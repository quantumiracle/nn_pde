{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN of 2D Navier-Stokes equation\n",
    "\n",
    "In this project we consider the following 2D Navier-Stokes equation,\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&u_t + \\lambda_1(u u_x + v u_y) = -p_x + \\lambda_2 (u_{xx} + u_{yy})\n",
    "\\\\\n",
    "&v_t + \\lambda_1(u v_x + v v_y) = -p_y + \\lambda_2 (v_{xx} + v_{yy})\n",
    "\\\\\n",
    "&u_x+v_y=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Here $u$, $v$ are two components of the velocity field, $p$ is the pressure field. The ground truth values of $\\lambda_1$ and $\\lambda_2$ are $1$ and $0.01$ respectively. The ground truth values of $u$, $v$, $p$ are stored in 'cylinder_nektar_wake.mat'.\n",
    "\n",
    "We solve this equation by PINN method. In this method, a neural network is introduced to approximate the solution and parameters in the network is find by the gradient descent method.\n",
    "\n",
    "To simplify the equation, we introduce the so called stream function $\\psi$. Actually, the third equation in above can be solved. All solutions $u$, $v$ of it are of the form:\n",
    "\\begin{equation}\n",
    "u= -\\psi_y,\\qquad v=\\psi_x\n",
    "\\end{equation}\n",
    "\n",
    "Therefore the Navier-Stokes equation is an equation of $\\psi$ and $p$.\n",
    "\n",
    "Finally we solve the following equation of $\\psi$ and $p$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&u_t + \\lambda_1(u u_x + v u_y) = -p_x + \\lambda_2 (u_{xx} + u_{yy})\n",
    "\\\\\n",
    "&v_t + \\lambda_1(u v_x + v v_y) = -p_y + \\lambda_2 (v_{xx} + v_{yy})\n",
    "\\\\\n",
    "&u= -\\psi_y,\\qquad v=\\psi_x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Substituting the last two equations into the first two removes $u$ and $v$ completely.\n",
    "\n",
    "Now let's go into the detail of the code, let's first import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "# from scipy.interpolate import griddata\n",
    "import time\n",
    "# from itertools import product, combinations\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "# # from plotting import newfig, savefig\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# from matplotlib import rc\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(100)\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the neural network are provided below. Before go into the code, we provide some remarks.\n",
    "\n",
    "1. **Organization of the data, the `__init__` function.** The data `xyt`, `u`, `v` are stored in 'cylinder_nektar_wake.mat' and their code can be find in the next code block. `xyt` is the spacetime location of the sample points and `u`, `v` are values of velocity fields components at these sample points. When sampling the data points, we first choose `T` time slices $\\{t_j\\}_{j=1,\\cdots,T}$ and on each time slice we choose `N` points $\\{(x_i, y_i)\\}_{i=1,\\cdots,N}$, finally on each data point $(x_i, y_i,t_j)$ we are given $u_{ij}=u(x_i, y_i,t_j)$ and $v_{ij}=v(x_i, y_i,t_j)$. \n",
    "\n",
    "2. **`initialize_NN` and `xavier_init` function.** We randomly initialize the weights the neural networks, using the Xavier initialization method, which sets each entry of the weights matrices `weights[i]` to be a Gaussian variable of mean $0$ and standard deviation $\\sqrt{\\frac{2}{in\\_ dim+out\\_ dim}}$ (assume that `weights[i].size()=[in_dim, out_dim]`).\n",
    "\n",
    "\n",
    "3. **`neural_net` and `net_NS` function.** `neural_net` defines a fully connected neural network and the size of each layer is given by `layers=[3,layers[1],...,layers[i-1], 2]`. When calling `neural_net` in `net_NS`, the input of this network is a `[N*T,3]` tensor `xyt` and the output is a `[N*T,2]` tensor `psi_and_p`. The first components of the output `psi_and_p[:,0]` is $\\psi$ and The first components `psi_and_p[:,1]` is $p$. The output of `net_NS` is `u`, `v`, `p`, `f_u`, `f_v` which is used to calculate the loss function. Remember that $u$ and $v$ can be calculated from $\\psi$ by $u= -\\psi_y$, $v=\\psi_x$. We take this derivative using numerical derivative $f'(x)=\\frac{f(x+\\Delta)-f(x)}{\\Delta}$. In the code `u = (self.neural_net(xyt+Delta*e_y, self.weights, self.biases) - self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta`, `self.neural_net(xyt+Delta*e_y, self.weights, self.biases)[:,0]` calculates $\\psi(x+\\Delta)$.\n",
    "\n",
    "4. **`loss_function` function.** The first two terms are data losses and the second two terms are equation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import backward\n",
    "\n",
    "class PhysicsInformedNN(nn.Module):\n",
    "    def __init__(self, xyt, u, v, layers): # xyt.size()=(N*T,3), Xbatch=N*T\n",
    "        super().__init__()\n",
    "        self.xyt = xyt\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        \n",
    "        self.lb = xyt.min()\n",
    "        self.ub = xyt.max()\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        self.weights, self.biases = self.initialize_NN(layers)  \n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = nn.Parameter(torch.zeros(1, dtype=torch.float64))\n",
    "        self.lambda_2 = nn.Parameter(torch.zeros(1, dtype=torch.float64))\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = nn.Parameter(torch.zeros([layers[l+1]], dtype=torch.float64))\n",
    "            weights.append(W)\n",
    "            biases.append(b) \n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return nn.Parameter(xavier_stddev*torch.randn([in_dim, out_dim], dtype=torch.float64))\n",
    "    \n",
    "    def neural_net(self, X, weights, biases): # X.size()=(Xbatch,3), \n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0     \n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = torch.tanh(H@W+b)\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = H@W+b\n",
    "        return Y\n",
    "    \n",
    "    def net_NS(self, xyt): # xyt.size()=(N*T,3) Xbatch=N*T\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        \n",
    "        psi_and_p = self.neural_net(xyt, self.weights, self.biases)\n",
    "        psi = psi_and_p[:,0] # psi.size(), p.size()=N*T\n",
    "        p = psi_and_p[:,1]\n",
    "\n",
    "        Delta = 0.001\n",
    "        \n",
    "        # calculate u,v, p_x, p_y\n",
    "        e_x = torch.tensor([[1,0,0]]).tile((xyt.size(0),1))\n",
    "        e_y = torch.tensor([[0,1,0]]).tile((xyt.size(0),1))\n",
    "        e_t = torch.tensor([[0,0,1]]).tile((xyt.size(0),1))\n",
    "        u = (self.neural_net(xyt+Delta*e_y, self.weights, self.biases) -\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta # u = psi_y\n",
    "        v = -(self.neural_net(xyt+Delta*e_x, self.weights, self.biases) -\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta # v = -psi_x\n",
    "        \n",
    "        p_x = (self.neural_net(xyt+Delta*e_x, self.weights, self.biases) -\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,1]/Delta\n",
    "        p_y = (self.neural_net(xyt+Delta*e_y, self.weights, self.biases) -\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,1]/Delta\n",
    "        p_t = (self.neural_net(xyt+Delta*e_t, self.weights, self.biases) -\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta\n",
    "        \n",
    "        \n",
    "        # calculate u_x, u_y, u_t; v_x, v_y, v_t\n",
    "        \n",
    "        u_x = (self.neural_net(xyt+Delta*e_x+Delta*e_y, self.weights, self.biases) - \\\n",
    "            self.neural_net(xyt+Delta*e_x, self.weights, self.biases)-\\\n",
    "            self.neural_net(xyt+Delta*e_y, self.weights, self.biases)+\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # u_x = psi_xy\n",
    "        u_y = (self.neural_net(xyt+Delta*e_y, self.weights, self.biases) + \\\n",
    "            self.neural_net(xyt-Delta*e_y, self.weights, self.biases)-\\\n",
    "            2*self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # u_y = psi_yy\n",
    "        u_t = (self.neural_net(xyt+Delta*e_y+Delta*e_t, self.weights, self.biases) - \\\n",
    "            self.neural_net(xyt+Delta*e_y, self.weights, self.biases)-\\\n",
    "            self.neural_net(xyt+Delta*e_t, self.weights, self.biases)+\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # u_t = psi_ty\n",
    "\n",
    "\n",
    "        \n",
    "        v_x = - (self.neural_net(xyt+Delta*e_x, self.weights, self.biases) + \\\n",
    "            self.neural_net(xyt-Delta*e_x, self.weights, self.biases)-\\\n",
    "            2*self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # v_x = - psi_xx\n",
    "        v_y = - (self.neural_net(xyt+Delta*e_x+Delta*e_y, self.weights, self.biases) - \\\n",
    "            self.neural_net(xyt+Delta*e_x, self.weights, self.biases)-\\\n",
    "            self.neural_net(xyt+Delta*e_y, self.weights, self.biases)+\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # v_x = - psi_xy\n",
    "        v_t = - (self.neural_net(xyt+Delta*e_x+Delta*e_t, self.weights, self.biases) - \\\n",
    "            self.neural_net(xyt+Delta*e_x, self.weights, self.biases)-\\\n",
    "            self.neural_net(xyt+Delta*e_t, self.weights, self.biases)+\\\n",
    "            self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # v_x = - psi_tx\n",
    "        \n",
    "        # calculate u_xx, u_yy; v_xx, v_yy\n",
    "        \n",
    "        u_xx = (self.neural_net(xyt+Delta*e_x+Delta*e_y, self.weights, self.biases)+\\\n",
    "            self.neural_net(xyt-Delta*e_x+Delta*e_y, self.weights, self.biases)- \\\n",
    "            2*self.neural_net(xyt+Delta*e_y, self.weights, self.biases)-\n",
    "            self.neural_net(xyt+Delta*e_x, self.weights, self.biases)-\\\n",
    "            self.neural_net(xyt-Delta*e_x, self.weights, self.biases)+\\\n",
    "            2*self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # u_xx = psi_xxy\n",
    "\n",
    "        u_yy = (0.5*self.neural_net(xyt+2*Delta*e_y, self.weights, self.biases) - \\\n",
    "            self.neural_net(xyt+Delta*e_x, self.weights, self.biases)+\\\n",
    "                self.neural_net(xyt+Delta*e_y, self.weights, self.biases)-\\\n",
    "            0.5*self.neural_net(xyt-2*Delta*e_y, self.weights, self.biases) )[:,0]/Delta**2 # u_yy = psi_yyy\n",
    "        \n",
    "        v_xx = -(0.5*self.neural_net(xyt+2*Delta*e_x, self.weights, self.biases) - \\\n",
    "            self.neural_net(xyt+Delta*e_x, self.weights, self.biases)+\\\n",
    "                self.neural_net(xyt+Delta*e_x, self.weights, self.biases)-\\\n",
    "            0.5*self.neural_net(xyt-2*Delta*e_x, self.weights, self.biases) )[:,0]/Delta**2 # v_xx = - psi_xxx\n",
    "            \n",
    "        v_yy = -(self.neural_net(xyt+Delta*e_x+Delta*e_y, self.weights, self.biases)+\\\n",
    "            self.neural_net(xyt+Delta*e_x-Delta*e_y, self.weights, self.biases)- \\\n",
    "            2*self.neural_net(xyt+Delta*e_x, self.weights, self.biases)-\n",
    "            self.neural_net(xyt+Delta*e_y, self.weights, self.biases)-\\\n",
    "            self.neural_net(xyt-Delta*e_y, self.weights, self.biases)+\\\n",
    "            2*self.neural_net(xyt, self.weights, self.biases) )[:,0]/Delta**2 # v_yy = - psi_xyy\n",
    "        \n",
    "        # calculate two components of f\n",
    "        f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy) \n",
    "        f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy)\n",
    "        \n",
    "        return u, v, p, f_u, f_v\n",
    "    \n",
    "    def forward(self, xyt):\n",
    "        return self.net_NS(xyt)\n",
    "    \n",
    "    def loss_function(self):\n",
    "        return  (self.u.squeeze() - self.forward(self.xyt)[0]).pow(2).sum()+\\\n",
    "                (self.v.squeeze() - self.forward(self.xyt)[1]).pow(2).sum()+\\\n",
    "                self.forward(self.xyt)[3].pow(2).sum()+\\\n",
    "                self.forward(self.xyt)[4].pow(2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another version of `net_NS` function use higher order derivative in pytorch. As you can see, the result of pytorch derivative `u[65]` and numerical derivative `u_diff[65]` are completely different but `u[65]` and `u_diff[65]` are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class PhysicsInformedNN_pytorch(PhysicsInformedNN):        \n",
    "        def net_NS(self, xyt): # xyt.size()=(N*T,3) Xbatch=N*T\n",
    "                self.create_graph = True\n",
    "                self.retain_graph = True\n",
    "                delta = 1e-7\n",
    "\n",
    "                lambda_1 = self.lambda_1\n",
    "                lambda_2 = self.lambda_2\n",
    "\n",
    "                psi_and_p = self.neural_net(xyt, self.weights, self.biases)\n",
    "                psi = psi_and_p[:,0] # psi.size(), p.size()=N*T\n",
    "                p = psi_and_p[:,1]\n",
    "\n",
    "                # calculate u,v, p_x, p_y\n",
    "                Dpsi = grad(psi, xyt, grad_outputs=torch.ones_like(psi), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "                Dp = grad(p, xyt, grad_outputs=torch.ones_like(p), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "\n",
    "                psi_x = Dpsi[0][:,0]\n",
    "                psi_y = Dpsi[0][:,1]\n",
    "                psi_t = Dpsi[0][:,2]\n",
    "                u = psi_y\n",
    "                e_0 = torch.tensor([[0,1,0]]).tile((xyt.size(0),1))\n",
    "                u_diff=1./delta*(self.neural_net(xyt+delta*e_0, self.weights, self.biases) -\\\n",
    "                        self.neural_net(xyt, self.weights, self.biases) )[:,0]\n",
    "#                 print(\"Total difference between numerical derivative and pytorch derivative: %e\" % ((u_diff-u).pow(2).sum().squeeze()))\n",
    "#                 for idx, (u_d_i, u_i) in enumerate(zip(u_diff, u)):\n",
    "#                     if torch.abs(u_d_i - u_i)>1e-1:\n",
    "#                         print(f\"u_diff-u: {idx}, {u_d_i-u_i}, {u_d_i:.6f}, {u_i:.6f}\")\n",
    "\n",
    "#                 print(\"u_diff[65]-u[65]: %e\" %(u_diff[65]-u[65]) )\n",
    "#                 print(\"u_diff[65]: %e\" %(u_diff[65]))\n",
    "#                 print(\"u[65]: %e\" %(u[65]))\n",
    "#                 print(\"u_diff[66]-u[66]: %e\" %(u_diff[66]-u[66]) )\n",
    "#                 print(\"u_diff[66]: %e\" %(u_diff[66]))\n",
    "#                 print(\"u[66]: %e\" %(u[66]))\n",
    "\n",
    "\n",
    "                v = -psi_x\n",
    "\n",
    "                p_x = Dp[0][:,0]\n",
    "                p_y = Dp[0][:,1]\n",
    "                p_t = Dp[0][:,2]\n",
    "\n",
    "\n",
    "                # calculate u_x, u_y, u_t; v_x, v_y, v_t\n",
    "                Du = grad(u, xyt, grad_outputs=torch.ones_like(u), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "                Dv = grad(v, xyt, grad_outputs=torch.ones_like(v), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "\n",
    "                u_x = Du[0][:,0]\n",
    "                u_y = Du[0][:,1]\n",
    "                u_t = Du[0][:,2]\n",
    "\n",
    "                v_x = Dv[0][:,0]\n",
    "                v_y = Dv[0][:,1]\n",
    "                v_t = Dv[0][:,2]\n",
    "\n",
    "                # calculate u_xx, u_yy; v_xx, v_yy\n",
    "                Du_x = grad(u_x, xyt, grad_outputs=torch.ones_like(u), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "                Du_y = grad(u_y, xyt, grad_outputs=torch.ones_like(u), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "                Dv_x = grad(v_x, xyt, grad_outputs=torch.ones_like(u), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "                Dv_y = grad(v_y, xyt, grad_outputs=torch.ones_like(u), create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "\n",
    "                u_xx = Du_x[0][:,0]\n",
    "                u_yy = Du_y[0][:,1]\n",
    "\n",
    "                v_xx = Dv_x[0][:,0]\n",
    "                v_yy = Dv_y[0][:,1]\n",
    "\n",
    "                # calculate two components of f\n",
    "                f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy) \n",
    "                f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy)\n",
    "                \n",
    "                return u, v, p, f_u, f_v\n",
    "        \n",
    "        def forward(self, xyt):\n",
    "                return self.net_NS(xyt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code block below, we prepare the training data `xyt` that will be the input of the network.  \n",
    "\n",
    "This `xyt` is a tensor of the size (N*T, 3), `xyt[i]` is of the form `tensor([x_i, y_i, t_i])`. In `xyt` x coordinate changes first, y coordinate changes second and z coordinate changes last. One example of `xyt` is `xyt=tensor([[1.0, 1.0, 1.0], [1.5, 1.0, 1.0], [2.0, 1.0, 1.0], [1.0, 1.5, 1.0], [1.5, 1.5, 1.0], [2.0, 1.5, 1.0], [1.0, 2.0, 1.0], [1.5, 2.0, 1.0], [2.0, 2.0, 1.0], [1.0, 1.0, 1.5], [1.5, 1.0, 1.5], [2.0, 1.0, 1.5], [1.0, 1.5, 1.5], [1.5, 1.5, 1.5], [2.0, 1.5, 1.5], [1.0, 2.0, 1.5], [1.5, 2.0, 1.5], [2.0, 2.0, 1.5], [1.0, 1.0, 2.0], [1.5, 1.0, 2.0], [2.0, 1.0, 2.0], [1.0, 1.5, 2.0], [1.5, 1.5, 2.0], [2.0, 1.5, 2.0], [1.0, 2.0, 2.0], [1.5, 2.0, 2.0], [2.0, 2.0, 2.0]])`.\n",
    "\n",
    "**It seems better to use the load data module in pytorch instead write by ourselves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and processing\n",
    "data = scipy.io.loadmat('./data/cylinder_nektar_wake.mat')\n",
    "\n",
    "U_star = data['U_star'] # N x 2 x T; full data of (u, v) at all x, y, t\n",
    "P_star = data['p_star'] # N x T; full data of p at all x, y, t\n",
    "t_star = data['t'] # T x 1; t corrdinates of the full data\n",
    "XY_star = data['X_star'] # N x 2; (x,y) coordinates of the full data\n",
    "\n",
    "N = XY_star.shape[0] # N=5000; size of the full data at a fixed t\n",
    "T = t_star.shape[0] # T=200; size of the full data at a fixed x,y\n",
    "\n",
    "\n",
    "# Definition of u, v, p\n",
    "u = torch.tensor(U_star[:,0,:].flatten()[:,None]) # NT x 1\n",
    "v = torch.tensor(U_star[:,1,:].flatten()[:,None]) # NT x 1\n",
    "p = torch.tensor(P_star.flatten()[:,None]) # NT x 1\n",
    "\n",
    "# definition of xyt, u, v, p\n",
    "xy = torch.tensor(XY_star, requires_grad=True)\n",
    "t = torch.tensor(t_star, requires_grad=True)\n",
    "\n",
    "xyxy = torch.cat([torch.tile(xy[:,0:1],(1,T)).flatten().unsqueeze(-1),torch.tile(xy[:,1:2],(1,T)).flatten().unsqueeze(-1)],-1)# xy.size()=(N,2)\n",
    "tt = torch.tile(t.unsqueeze(0), (N,1)).flatten().unsqueeze(1) # t.size()=(N*T,1)\n",
    "\n",
    "xyt = torch.cat([xyxy,tt], 1) # NT x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(pinn, optim_method, nIter, lr): \n",
    "    start_time = time.time()\n",
    "#     parameter_list = (pinn.weights + pinn.biases + [param for param in pinn.parameters()]).__iter__()\n",
    "    parameter_list = pinn.weights + pinn.biases + [param for param in pinn.parameters()]\n",
    "    if optim_method == \"adam\":\n",
    "        optim = torch.optim.Adam(parameter_list,lr)\n",
    "    elif optim_method == \"sgd\":\n",
    "        optim = torch.optim.SGD(parameter_list,lr)\n",
    "    else:\n",
    "        print(\"THIS OPTIMIZATION METHOD IS NOT SUPPORTED.\")\n",
    "        return 0\n",
    "    for it in range(nIter):\n",
    "        # Print\n",
    "        loss_value = pinn.loss_function()\n",
    "        # loss_value_reg = pinn.loss_function_reg()\n",
    "        optim.zero_grad()\n",
    "        # loss_value_reg.backward(retain_graph=True)\n",
    "        loss_value.backward(retain_graph=True, inputs=parameter_list)\n",
    "        optim.step()\n",
    "        lambda_1_value = pinn.lambda_1\n",
    "        lambda_2_value = pinn.lambda_2\n",
    "        if it<10 or it % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Loss: %.3e, l1: %.3f, l2: %.5f, Time: %.2f' % \n",
    "                  (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
    "            print(pinn.weights[0].grad.pow(2).sum())\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code block below, we train the pinn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 1.826e+03, l1: -0.001, l2: -0.00100, Time: 0.27\n",
      "tensor(362593.4730, dtype=torch.float64)\n",
      "It: 1, Loss: 1.778e+03, l1: -0.002, l2: -0.00176, Time: 0.27\n",
      "tensor(288477.6679, dtype=torch.float64)\n",
      "It: 2, Loss: 1.743e+03, l1: -0.002, l2: -0.00217, Time: 0.34\n",
      "tensor(342281.1255, dtype=torch.float64)\n",
      "It: 3, Loss: 1.708e+03, l1: -0.003, l2: -0.00236, Time: 0.29\n",
      "tensor(338497.6284, dtype=torch.float64)\n",
      "It: 4, Loss: 1.670e+03, l1: -0.003, l2: -0.00241, Time: 0.28\n",
      "tensor(278975.2193, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Training Process\n",
    "N_train = 2000 #5000\n",
    "N_test = 1000\n",
    "    \n",
    "layers = [3, 100, 50, 20, 2]\n",
    "#[3, 20, 20, 20, 20, 20, 20, 20, 20, 2]\n",
    "#[3, 300, 200, 100, 2]\n",
    "\n",
    "nIter = 5 #2000  # original niter is 200000\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "optim_method = \"adam\"\n",
    "\n",
    "# Training Data    \n",
    "idx = np.random.choice(N*T, N_train, replace=False) # Generate a random sample from np.arange(N*T) of size N_train without replacement\n",
    "xyt_train = xyt[idx,:]\n",
    "u_train = u[idx,:]\n",
    "v_train = v[idx,:]\n",
    "\n",
    "# Training\n",
    "pinn = PhysicsInformedNN_pytorch(xyt_train, u_train, v_train, layers) \n",
    "train(pinn, optim_method, nIter, lr)\n",
    "\n",
    "# Test Data\n",
    "idx = np.random.choice(N*T, N_test, replace=False) # Generate a random sample from np.arange(N*T) of size N_train without replacement\n",
    "xyt_test = xyt[idx,:]\n",
    "u_test = u[idx,:]\n",
    "v_test = v[idx,:]\n",
    "\n",
    "# Prediction\n",
    "u_pred, v_pred, p_pred = pinn(xyt_test)[0:3]\n",
    "lambda_1_value = pinn.lambda_1\n",
    "lambda_2_value = pinn.lambda_2\n",
    "\n",
    "# Error\n",
    "# error_u = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "# error_v = np.linalg.norm(v_test-v_pred,2)/np.linalg.norm(v_test,2)\n",
    "# error_p = np.linalg.norm(p_test-p_pred,2)/np.linalg.norm(p_test,2)\n",
    "\n",
    "# error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "# error_lambda_2 = np.abs(lambda_2_value - 0.01)/0.01 * 100\n",
    "\n",
    "# print('Error u: %e' % (error_u))    \n",
    "# print('Error v: %e' % (error_v))    \n",
    "# print('Error p: %e' % (error_p))    \n",
    "# print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "# print('Error l2: %.5f%%' % (error_lambda_2))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
